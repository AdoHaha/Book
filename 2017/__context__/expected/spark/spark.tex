\usemodule[pycon-yyyy]
\starttext


\section[welcome-to-the-first-steps-with-pyspark]{Welcome to the
\quote{First steps with PySpark}!}

\section[what-is-apache-spark]{What is Apache Spark?}

\startblockquote
Apache Spark is a fast and general engine for large-scale data
processing
\stopblockquote

\startblockquote
Run programs up to 100x faster than Hadoop MapReduce in memory, or 10x
faster on disk.
\stopblockquote

Project page: {\em https://spark.apache.org/}

\section[what-is-pyspark]{What is PySpark?}

\startblockquote
PySpark is the Spark Python API, which exposes the Spark programming
model to Python.
\stopblockquote

Docs: https://spark.apache.org/docs/latest

\section[spark-ecosystem]{Spark ecosystem}

\startitemize[packed]
\item
  Spark SQL (for structured data)
\item
  Spark Streaming (real time data)
\item
  MLib (machine learning library)
\item
  GraphX (graph processing)
\stopitemize

\section[main-spark-concept]{Main Spark concept}

\startitemize[packed]
\item
  DataFrames (stores your data)
\item
  transformations (transform your data)
\item
  actions (generate results)
\stopitemize

\section[spark-context-and-sql-context]{Spark context and SQL
context}

In order to use Spark and its DataFrame API we will need to use a
SQLContext. When running Spark, you start a new Spark application by
creating a SparkContext. You can then create a SQLContext from the
SparkContext. When the SparkContext is created, it asks the master for
some cores to use to do work. The master sets these cores aside just for
you; they won't be used for other applications.

\starttyping
# Display the type of the Spark sqlContext
type(sqlContext)


# List sqlContext's attributes
dir(sqlContext)


# Use help to obtain more detailed information
help(sqlContext)
\stoptyping

Outside of pyspark or a notebook, SQLContext is created from the
lower-level SparkContext, which is usually used to create Resilient
Distributed Datasets (RDDs). An RDD is the way Spark actually represents
data internally; DataFrames are actually implemented in terms of RDDs.

While you can interact directly with RDDs, DataFrames are preferred.
They're generally faster, and they perform the same no matter what
language (Python, R, Scala or Java) you use with Spark.

In this course, we'll be using DataFrames, so we won't be interacting
directly with the Spark Context object very much. However, it's worth
knowing that inside pyspark or a notebook, you already have an existing
SparkContext in the sc variable. One simple thing we can do with sc is
check the version of Spark we're using:

\section[dataframe]{DataFrame}

DataFrame is a 2-dimensional labeled data structure. Is consists of a
series of Row objects; each Row object has a set of named columns. You
can think of a DataFrame as modeling a table, though the data source
being processed does not have to be a table. More formally, a DataFrame
must have a schema, which means it must consist of columns, each of
which has a name and a type.

On DataFrames we perform transformations and actions. DataFrame is
immutable, so once it is created, it cannot be changed. As a result,
each transformation creates a new DataFrame. Finally, we can apply one
or more actions to the DataFrames.

Spark uses lazy evaluation, so transformations are not actually executed
until an action occurs.

\subsection[prepare-testing-data]{Prepare testing data}

We will use a third-party Python testing library called
\type{fake-factory} to create our dataset - a collection of randomly
generated fake person records.

\starttyping
from faker import Factory
fake = Factory.create()
fake.seed(4321)


# Each entry consists of last_name, first_name, ssn, job, and age.
from pyspark.sql import Row
def fake_entry():
    name = fake.name().split()
    return Row(name[1], name[0], fake.ssn(), fake.job(), abs(2016 - fake.date_time().year) + 1)


# Create a helper function to call a function repeatedly
def repeat(times, func, *args, **kwargs):
    for _ in range(times):
        yield func(*args, **kwargs)


data = list(repeat(10000, fake_entry))
\stoptyping

Data is just a normal Python list, containing Spark SQL \type{Row}
objects. Let's look for details:

\starttyping
data[0][0], data[0][1], data[0][2], data[0][3], data[0][4]


# Check the size of the list
len(data)
\stoptyping

\subsection[create-dataframe]{Create DataFrame}

To create DataFrame we will use \type{createDataFrame} method:

\starttyping
help(sqlContext.createDataFrame)


data_df = sqlContext.createDataFrame(data, ('last_name', 'first_name', 'ssn', 'occupation', 'age'))


print('type of data_df: {0}'.format(type(data_df)))


data_df.printSchema()


sqlContext.registerDataFrameAsTable(data_df, 'dataframe')


# What methods can we call on this DataFrame?
help(data_df)


# How many partitions will the DataFrame be split into?
data_df.rdd.getNumPartitions()


new_df = data_df.distinct().select('*')
new_df.explain(True)
\stoptyping

\subsection[working-on-dataframe]{Working on DataFrame}

\starttyping
# Transform data_df through a select transformation and rename the newly created '(age -1)' column to 'age'
# Because select is a transformation and Spark uses lazy evaluation, no jobs, stages,
# or tasks will be launched when we run this code.
sub_df = data_df.select('last_name', 'first_name', 'ssn', 'occupation', (data_df.age - 1).alias('age'))


# Query plan
sub_df.explain(True)
\stoptyping

\section[actions]{Actions}

\startblockquote
Action operations cause Spark to perform the (lazy) transformation
operations that are required to compute the values returned by the
action.
\stopblockquote

\subsection[collect]{collect()}

\startblockquote
Return all the elements of the dataset as an array at the driver
program. This is usually useful after a filter or other operation that
returns a sufficiently small subset of the data.
\stopblockquote

\startblockquote
Be careful while expecting a big amount of results. The data returned to
the driver must fit into the driver's available memory. If not, the
driver will crash.
\stopblockquote

\starttyping
# Use collect to view result
results = sub_df.collect()
print(results)
\stoptyping

\subsection[show]{show()}

\startblockquote
Another (usually better) way to visualize the data. If you don't tell
\type{show()} how many rows to display, it displays 20 rows.
\stopblockquote

\starttyping
sub_df.show()
\stoptyping

If you'd prefer that show() not truncate the data, you can tell it not
to:

\starttyping
sub_df.show(n=30, truncate=False)
\stoptyping

In Databricks, there's an even nicer way to look at the values in a
DataFrame: The display() helper function.

\starttyping
display(sub_df)
\stoptyping

\subsection[count]{count()}

\startblockquote
Returns the number of elements in the dataset.
\stopblockquote

\starttyping
# Counting records
print(data_df.count())
print(sub_df.count())
\stoptyping

\subsection[first-and-take]{first() and take()}

\startblockquote
One useful thing to do when we have a new dataset is to look at the
first few entries to obtain a rough idea of what information is
available. In Spark, we can do that using actions like \type{first()},
\type{take()}, and \type{show()}. Note that for the \type{first()} and
\type{take()} actions, the elements that are returned depend on how the
DataFrame is partitioned.
\stopblockquote

\startblockquote
Instead of using the \type{collect()} action, we can use the take(n)
action to return the first n elements of the DataFrame. The
\type{first()} action returns the first element of a DataFrame, and is
equivalent to \type{take(1)[0]}.
\stopblockquote

\starttyping
print("first: {0}\n".format(sub_df.first()))
print("Four of them: {0}\n".format(sub_df.take(4)))


display(sub_df.take(4))
\stoptyping

\section[transformations]{Transformations}

\subsection[filtering]{Filtering}

\startblockquote
The \type{filter()} method is a transformation operation that creates a
new DataFrame from the input DataFrame, keeping only values that match
the filter expression.
\stopblockquote

\startblockquote
An alias to \type{filter()} is \type{where()}
\stopblockquote

\starttyping
filtered_df = sub_df.filter(sub_df.age < 10)
filtered_df.show(truncate=False)
filtered_df.count()
\stoptyping

\subsection[user-defined-functions-udf]{User defined functions
(UDF)}

\startblockquote
To pass function over our DataFrame we can define user defined function
(UDF). UDF is a special wrapper around a function, allowing the function
to be used in a DataFrame query.
\stopblockquote

\startblockquote
Also \type{lambda} functions and \type{map}, \type{reduce} operations
are often useful.
\stopblockquote

\starttyping
from pyspark.sql.types import BooleanType
less_ten = udf(lambda s: s < 10, BooleanType())
lambda_df = sub_df.filter(less_ten(sub_df.age))
lambda_df.show()
lambda_df.count()


# Let's collect the even values less than 10
even = udf(lambda s: s % 2 == 0, BooleanType())
even_df = lambda_df.filter(even(lambda_df.age))
even_df.show()
even_df.count()
\stoptyping

\subsection[orderby]{orderBy()}

\startblockquote
orderBy() allows you to sort a DataFrame by one or more columns,
producing a new DataFrame.
\stopblockquote

\startblockquote
orderBy() takes one or more columns, either as names (strings) or as
Column objects. To get a Column object, we use one of two notations on
the DataFrame: Pandas-style notation: filtered\type{_}df.age Subscript
notation: filtered\type{_}df{[}\quote{age}{]}
\stopblockquote

\starttyping
# Get the five oldest people in the list. To do that, sort by age in descending order.
display(dataDF.orderBy(dataDF.age.desc()).take(5))


display(dataDF.orderBy('age').take(5))
\stoptyping

\subsection[distinct-and-dropduplicates]{distinct() and
dropDuplicates()}

\startblockquote
distinct() filters out duplicate rows, and it considers all columns.
\stopblockquote

\startblockquote
dropDuplicates() is like distinct(), except that it allows us to specify
the columns to compare.
\stopblockquote

\starttyping
print(data_df.count())
print(data_df.distinct().count())


print data_df.dropDuplicates(['first_name', 'last_name']).count()
\stoptyping

\subsection[drop]{drop()}

\startblockquote
drop() is like the opposite of select(): Instead of selecting specific
columns from a DataFrame, it drops a specified column from a DataFrame.
\stopblockquote

Here's a simple use case: Suppose you're reading from a 1,000-column CSV
file, and you have to get rid of five of the columns. Instead of
selecting 995 of the columns, it's easier just to drop the five you
don't want.

\starttyping
data_df.drop('occupation').drop('age').show()
\stoptyping

\subsection[groupby]{groupBy()}

\startblockquote
groupBy() allows you to perform aggregations on a DataFrame.
\stopblockquote

\startblockquote
Unlike other DataFrame transformations, \type{groupBy()} does not return
a DataFrame. Instead, it returns a special GroupedData object that
contains various aggregation functions.
\stopblockquote

\startblockquote
The most commonly used aggregation function is \type{count()}, but there
are others (like \type{sum()}, \type{max()}, and \type{avg()}.
\stopblockquote

\startblockquote
These aggregation functions typically create a new column and return a
new DataFrame.
\stopblockquote

\starttyping
data_df.groupBy('occupation').count().show(truncate=False)


data_df.groupBy().avg('age').show(truncate=False)


# We can also use groupBy() to do another useful aggregations:
print("Maximum age: {0}".format(data_df.groupBy().max('age').first()[0]))
print("Minimum age: {0}".format(data_df.groupBy().min('age').first()[0]))
\stoptyping

\subsection[sample]{sample()}

\startblockquote
Returns a new DataFrame with a random sample of elements from the
dataset. It takes in a \type{withReplacement} argument, which specifies
whether it is okay to randomly pick the same item multiple times from
the parent DataFrame (so when \type{withReplacement=True}, you can get
the same item back multiple times). It takes in a \type{fraction}
parameter, which specifies the fraction elements in the dataset you want
to return. (So a fraction value of 0.20 returns 20\letterpercent{} of
the elements in the DataFrame.) It also takes an optional \type{seed}
parameter that allows you to specify a seed value for the random number
generator, so that reproducible results can be obtained.
\stopblockquote

\starttyping
sampled_df = data_df.sample(withReplacement=False, fraction=0.10)
print sampled_df.count()
sampled_df.show()


print(data_df.sample(withReplacement=False, fraction=0.05).count())
\stoptyping

\section[caching-dataframes]{Caching DataFrames}

For efficiency Spark keeps your DataFrames in memory, so it can quickly
access the data. However, memory is limited - if you try to keep too
many partitions in memory, Spark will automatically delete partitions
from memory to make space for new ones. If you later refer to one of the
deleted partitions, Spark will automatically recreate it for you, but
that takes time.

So, if you plan to use a DataFrame more than once, then you should tell
Spark to cache it. You can use the \type{cache()} operation to keep the
DataFrame in memory. However, you must still trigger an action on the
DataFrame, such as \type{collect()} or \type{count()} before the caching
will occur.

\starttyping
# Cache the DataFrame
filtered_df.cache()
# Trigger an action
print(filtered_df.count())
# Check if it is cached
print(filtered_df.is_cached)
\stoptyping

For efficiency, once you are finished using cached DataFrame, you can
optionally tell Spark to stop caching it in memory by using the
DataFrame's \type{unpersist()} method to inform Spark that you no longer
need the cached data.

\starttyping
# If we are done with the DataFrame we can unpersist it so that its memory can be reclaimed
filtered_df.unpersist()
# Check if it is cached
print(filtered_df.is_cached)
\stoptyping

\section[debugging]{Debugging}

Internally, Spark executes using a Java Virtual Machine (JVM). pySpark
runs Python code in a JVM using Py4J. Py4J enables Python programs
running in a Python interpreter to dynamically access Java objects in a
Java Virtual Machine. Methods are called as if the Java objects resided
in the Python interpreter and Java collections can be accessed through
standard Python collection methods. Py4J also enables Java programs to
call back Python objects. This implies that coding errors often result
in a complicated, confusing stack trace that can be difficult to
understand.

Spark's use of lazy evaluation can make debugging more difficult because
code is not always executed immediately.

\starttyping
def broken_ten(value):
    """
    Incorrect implementation of the ten function
    (the `if` statement checks an undefined variable `val` instead of `value`).
    :param value: a number.
    :return bool: whether `value` is less than 10.

    The function references `val`, which is not available in the local or global
    namespace, so a `NameError` is raised.
    """
    return True if (val < 10) else False

bt_udf = udf(broken_ten)
broken_df = sub_df.filter(bt_udf(sub_df.age) == True)


# Now we'll see the error
# Click on the `+` button to expand the error and scroll through the message.
broken_df.count()
\stoptyping

\subsection[coding-style]{Coding style}

To make your coding style more readable, enclose the statement in
parentheses and put each method, transformation, or action on a separate
line.

\starttyping
# Final version
from pyspark.sql.functions import *
(data_df
 .filter(data_df.age > 20)
 .select(concat(data_df.first_name, lit(' '), dataDF.last_name), dataDF.occupation)
 .show(truncate=False)
 )
\stoptyping

\section[exercise]{EXERCISE}

Create DataFrame containing basic info (last\type{_}name,
first\type{_}name, ssn, job, age) about 20000 people.

\starttyping
# Solution here
\stoptyping

Remove column \quote{ssn}

\starttyping
# Solution here
\stoptyping

Display the number of different jobs

\starttyping
# Solution here
\stoptyping

What is the job of the oldest person?

\starttyping
# Solution here
\stoptyping

Display 10 most frequent first names with numbers of occurrences

\starttyping
# Solution here
\stoptyping

What is the most frequent job for people under 20?

\starttyping
# Solution here
\stoptyping

Display all rare jobs (jobs that occurs only once in our dataset)

\starttyping
# Solution here
\stoptyping

\section[io]{I/O}

We can create DataFrame reading data from file. > text file:
\type{sqlContext.read.text(file)}

\startblockquote
csv file: \type{sqlContext.read.csv(file)}
\stopblockquote

\startblockquote
json file: \type{sqlContext.read.json(file)}
\stopblockquote

\starttyping
filename = "pantadeusz.txt"
text_df = sqlContext.read.text(filename, encoding="utf-8")
text_df.show(15, truncate=False)
\stoptyping

\section[exercise-1]{EXERCISE}

Print 10 most common words in \type{Pan Tadeusz}.

Note: - we need first properly transform input data (remove
punctuations, trailing and leading spaces, change to lowercase)

\starttyping
# Solution here
\stoptyping


\stoptext
